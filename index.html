<!DOCTYPE html>
<html class="js no-touchevents svg imgcrossorigin target matchmedia" lang="en-US" style="">

<head>
    <link rel="icon" type="image/png" href="./ALI/Icon.png">

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>
        Applied Learning Initiative
    </title>


    <link rel="dns-prefetch" href="http://s.w.org/">
    <style type="text/css">
        img.wp-smiley,
        img.emoji {
          display: inline !important;
          border: none !important;
          box-shadow: none !important;
          height: 1em !important;
          width: 1em !important;
          margin: 0 .07em !important;
          vertical-align: -0.1em !important;
          background: none !important;
          padding: 0 !important;
        }
    </style>
    <link rel="stylesheet" id="ali-theme-css" href="./ALI/style.css" type="text/css" media="screen">
    <link rel="stylesheet" id="ali-theme-print-css" href="./ALI/print.css" type="text/css" media="print">
    <script type="text/javascript" src="./ALI/modernizr.js"></script>
    <script type="text/javascript" src="./ALI/jquery.js"></script>
    <script type="text/javascript" src="./ALI/jquery-migrate.min.js"></script>
    <script type="text/javascript" src="./ALI/jquery.json.min.js"></script>
    <script type="text/javascript" src="./ALI/gravityforms.min.js"></script>
    <script type="text/javascript" src="./ALI/placeholders.jquery.min.js"></script>
    <script type="text/javascript" src="./ALI/algoliasearch.jquery.min.js"></script>
    <script type="text/javascript" src="./ALI/autocomplete.min.js"></script>
    <script type="text/javascript" src="./ALI/tether.min.js"></script>
    <link rel="canonical" href="http://www.appliedlearninginitiative.org/">
    <link rel="shortlink" href="http://www.appliedlearninginitiative.org/">
    <script type="text/javascript">
		<!--//--><![CDATA[//><!--
			var images = new Array()
			function preload() {
				for (i = 0; i < preload.arguments.length; i++) {
					images[i] = new Image()
					images[i].src = preload.arguments[i]
				}
			}
			preload(
				"./ALI/Gallery/a.png",
				"./ALI/Gallery/b.png",
				"./ALI/Gallery/c.png",
                "./ALI/Gallery/d.png",
                "./ALI/Gallery/e.png"
			)
		//--><!]]>
	</script>

</head>

<body class="home page page-id-629 page-template-default" style="overflow-x: hidden;     scroll-behavior: smooth;
">
    <a name="top"></a>
    <div id="pjax-announce" aria-live="assertive" class="visually-hidden"></div>

    <div id="pjax-container" class="wrapper">

        <section class="color-theme theme--default">

            <header class="l--header feature-theme--salmon is--over-feature" role="banner">

                <div class="header-inner">

                    
                    <div class="site-branding">
                    <button style="border: none;" onclick="window.scrollTo({top:0,left: 0,behavior:'smooth'});">
                            <?xml version="1.0" encoding="UTF-8" standalone="no"?>
                            <svg viewBox="50 85 190 85" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" class="site-branding--svg">



 <g>
  <path stroke="#000" id="svg_1" d="m55.731812,168.071547l43.735124,-87.999997l43.735124,87.999997l-87.470249,0z" stroke-width="2.25" fill="#ffffff00"/>
  <rect stroke="#000" id="svg_2" height="86.999998" width="31.992158" y="81.071549" x="178.301165" fill-opacity="null" stroke-opacity="null" stroke-width="2.25" fill="#ffffff00"/>
  <path stroke="#000" id="svg_3" d="m115.536371,107.822094l0,-75.577482l62.519861,75.577482l-62.519861,0z" fill-opacity="null" stroke-opacity="null" stroke-width="2.25" fill="#ffffff00"/>
 </g>

        </svg></button>
                    <h1 class="site-branding--title"><a href="./" class="site-branding--link-title">Applied<span>Learning</span>Initiative</a></h1>
                    </div>

                    <nav class="site-nav">
                        <ul class="site-nav-list" role="toolbar">
                            <li class="site-nav-item"><button onclick="window.scrollTo({top:0,left: 0,behavior:'smooth'});" style="padding: 5px 8px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60">About</button></li>
                            <li class="site-nav-item"><button onclick="window.scrollTo({top:640,left: 0,behavior:'smooth'});" style="padding: 5px 8px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60">Projects</button></li>
                            <li class="site-nav-item"><button onclick="window.scrollTo({top:0,left: 0,behavior:'smooth'});" style="padding: 5px 8px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60">Contact</button></li>
                        </ul>
                    </nav>

                    <div id="site-nav-panels">
                        <nav class="l--menu site-nav--panel" id="menu" tabindex="-1">


                        </nav>


                    </div>

                </div>
                <!-- /.header-inner -->

            </header>
            <!-- /.l--header -->


            <section class="l--content" role="main" style="margin-bottom: -758px; padding-bottom: 758px;">

                <div class="base-header feature-header feature-theme--salmon">


                    <article class="base-feature-inner">

                        <header class="base-feature__text" style="width:40%; padding-right:6%; min-width:300px">








                            <h1 class="base-tease__heading post-tease__heading">
                                We connect Stanford students<br>to high-impact projects.
                            </h1>


                            <p class="base-tease__subheading" style="height:0.01em"></p>
                            <p class="base-tease__subheading">We connect students in computer science, mathematics, physics, statistics and other quantitative fields to high-impact projects. Students provide much needed expertise while gaining valuable experience with real-world projects.</p>
                            <div class="base-tease__subheading"><button style="padding: 0px 0px; font-family: sans-serif;font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3nSG30P2cMx_mNqR9oWO1zUCFAs82c2-kdJgVMwC3H9hgFA/viewform" target="_blank" style="text-decoration: none;">Subscribe</a></button> to hear about new projects.</div>

                            <div class="base-tease__subheading">Interested in providing a project? Submit <button style="padding: 0px 0px; font-family: sans-serif;font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSdtUUie_d-IUkvGlEuMv48SJdcNRCg3s5NFF14eIdX-75e94A/viewform?" target="_blank" style="text-decoration: none;">here</a></button>.</div>

                        
                        <div class="base-tease__subheading">Have questions or want to work with us?<br>Reach us at <button style="padding: 0px 0px; font-family: sans-serif;font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="mailto:info@appliedlearninginitiative.org" target="_blank" style="text-decoration: none;">info@appliedlearninginitiative.org</a></button>.</div>




                        </header>
                        <!-- /.base-feature__text -->


                        <div class="base-feature__image" style="max-width:500px">
                            <article class="l--tease l--project-tease tease-lg  l--format-stack tease-has-thumbnail " role="article">

                                <div class="base-tease-inner project-tease-inner">


                                    <div id="gallery" class="base-tease__image-container">
                                        <div class="base-tease__image-stack">
                                            <figure class="base-tease__figure project-tease__figure" style="height: 311px;">
                                                <img src="./ALI/Gallery/a.png" alt="" class="base-tease__image project-tease__image">
                                                <img src="./ALI/Gallery/d.png" alt="" class="base-tease__image project-tease__image">
                                                <img src="./ALI/Gallery/c.png" alt="" class="base-tease__image project-tease__image">
                                                <img src="./ALI/Gallery/b.png" alt="" class="base-tease__image project-tease__image">
                                                <img src="./ALI/Gallery/e.png" alt="" class="base-tease__image project-tease__image">
                                            </figure>
                                        </div>
                                        </a>
                                    </div>
                                    <!-- ./base-tease__image-container -->




                                </div>
                                <!-- /.base-tease-inner -->

                            </article>
                            <!-- /.l--article -->

                        </div>
                        <!-- /.base-feature__text -->



                    </article>
                    <!-- /.base-feature-inner -->

                </div>

                <div id="projects" class="l--main">

                    <div class="base-main-inner">

                        <div class="base-body project-body">
                            <center>
				<br>
                                <p class="partners">Partners</p>
                                <img src="./ALI/Partners/partners.png" style="width:100%;"><br>
                            </center>

                            <center>
                                <br><br>
                                <p class="partners">Leadership</p>
                                </center>
                                <text class="base-tease__subheading">
                                <div class="base-tease__subheading"><span style="padding: 0px 0px; font-family: sans-serif;font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;">President</span>&nbsp;&nbsp;&nbsp;Mika Sarkin Jain</div>
                                <div class="base-tease__subheading"><span style="padding: 0px 0px; font-family: sans-serif;font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;">Vice President</span>&nbsp;&nbsp;&nbsp;Tom Pritsky</div>
                                <div class="base-tease__subheading"><span style="padding: 0px 0px; font-family: sans-serif;font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;">Outreach</span>&nbsp;&nbsp;&nbsp;Matthew Prospero, Neha Srivathsa, Levi Lian, Jack Lindsey, Stelios Serghiou</div>
                                <div class="base-tease__subheading"><span style="padding: 0px 0px; font-family: sans-serif;font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;">Faculty Advisors</span>&nbsp;&nbsp;&nbsp;Professor Tarik Massoud, Professor Oliver Aalami</div>
                                <hr style="margin-top:-15px; visibility:hidden;" />

                                </text>
                            
			
			    <center>
				<br>
				<p class="partners"> <sup><font size=2 color="blue"><b>NEW</b></font></sup> Self-Designed Projects 2019–2020 </p>
		                <p class="base-tease__subheading">Design a project alongside faculty and researchers.<p>
                                <hr style="margin-top:-20px;margin-bottom:40px; visibility:hidden;" />
                            </center>
				
				
			<text><font size="5">Healthcare AI/ML Collaboratory in Cardiovascular Medicine</font></text><text class="base-tease__subheading">
                            <br>
                            Division of Cardiovascular Medicine, Department of Medicine, Stanford</text>
                            <p class="base-tease__subheading"><br>Interested in a collaboratively-developed project in cardiovascular medicine under guidance from Stanford Medical School faculty? Apply here to work with a faculty/fellow mentor to design a project from the ground up. Faculty mentor/advisors will assist student team conceptualize and design a project that is highly problem-focused and medically relevant from conception to completion. Depending on outcome of the class project, options for continuation of the project beyond a single quarter can be arranged. Ultimately, the goal will be to develop a AI/ML solution to a real-world biomedical/healthcare problem that can be implemented in near term. Commitment of one quarter or longer, depending on outcome. Students who are interested AI/ML projects in clinical area outside of cardiovascular medicine may also be considered on a case-by-case basis.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://forms.gle/uHuz2DYUawucWirS8" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

                            <center>
                                <p class="partners"> Open Projects 2019–2020 </p>
                                <p class="base-tease__subheading">Join a prexisting project in collaboration with faculty and researchers.<p>
				<hr style="margin-top:-20px;margin-bottom:40px; visibility:hidden;" />
                            </center>
				
			<text><font size="5">Prostate Cancer Grading on Histopathology Images</text><text class="base-tease__subheading">
                            <br>
                            Department of Radiology</text>
                            <p class="base-tease__subheading"><br>Although radiology images, such as MRI, are very useful to identify suspicious lesions, tissue samples from biopsy or surgery, remain the only way to diagnose cancer. A pathologist visually inspects the histopathology images of the tissue sample and assesses the presence of cancer, thereby diagnosing the disease, and categorizes its aggressiveness by providing a grade. There is a need to automate the diagnosis and grading of prostate cancer on histopathology images to limit the subject interpretation of images and allow for high throughput annotation of histopathology images. This project will use public data release as part of a MICCAI challenge and will focus on developing deep learning methods to distinguish different grades of prostate cancer based on histopathology images. The dataset includes 244 images with Gleason grades regional labels provided by 6 pathologists. Data and additional information: https://gleason2019.grand-challenge.org/</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://forms.gle/ARBrqjhZKeJeKawB7" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>
				
			<text><font size="5">Prostate Cancer Detection on MRI </text><text class="base-tease__subheading">
                            <br>
                            Department of Radiology</text>
                            <p class="base-tease__subheading"><br>Despite technical advancements in the diagnosis and treatment of prostate cancer, it remains the most common and the second most deadly cancer in men in US [1]. MRI plays an essential role in the diagnosis of prostate cancer, yet the radiologists’ interpretation of the MRI images has a relatively high sensitivity, 0.85-0.95, yet a reduced specificity, 0.71-0.75, to detect prostate cancer. There is a need to develop advanced machine learning methods to accurately detect cancer on MRI. This project will use public data released as part of the prostateX challenge and will focus on developing deep learning methods to distinguish benign conditions (non-cancer) from cancer on multi-parametric MRI. The training dataset includes 204 patients with 331 findings categorized as cancer or benign. Data and additional information: https://wiki.cancerimagingarchive.net/display/Public/SPIE-AAPM-NCI+PROSTATEx+Challenges#28ab80368ed440fb9a26afa8bbef487a.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://forms.gle/tCjJwUrDQZCDZuJF9" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

			<text><font size="5">Development of Brain-Based Biomarkers for Pain</font></text><text class="base-tease__subheading">
                            <br>
                            Division of Pain Medicine, Department of Anesthesiology, Perioperative and Pain Medicine</text>
                            <p class="base-tease__subheading"><br>There is an urgent need for the development of biomarkers of chronic pain to better predict (1) the best treatment response for an individual patient; and (2) whether a person’s pain will improve or worsen over time. We have been developing neuroimaging-based biomarkers of chronic pain and amassed a large dataset of brain and spinal cord neuroimaging scans of people with chronic pain and matched healthy controls. There are multiple projects involving the use of traditional and machine learning approaches for the development of prediction models. We ultimately wish to translate this knowledge into clinical decision support tools to realize the vision of precision pain medicine where can provide the best treatment for a particular patient under a specific circumstance. For those interested in applying to medical school or a health related PhD program, we have a long experience with providing shadowing at the Stanford Pain Management Center. We can help with applications and have a long track record in helping students get accepted into these programs. Furthermore, plenty of opportunities for students to either lead or participate in projects leading to a manuscript publication. For more information on our research group please go to: http://snapl.stanford.edu</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://forms.gle/MTPiZ3XnQQ1yw5Qi8" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>
				
			<text><font size="5">Optimizing Patient Care and Innovative Research Using an Open Source Learning Healthcare System</font></text><text class="base-tease__subheading">
                            <br>
                            Division of Pain Medicine, Department of Anesthesiology, Perioperative and Pain Medicine</text>
                            <p class="base-tease__subheading"><br>Chronic pain is an astounding problem affecting 100 million Americans and costing our country half a trillion dollars per year. One of the major challenges in understanding and treating pain is the need for better quality data. To address this problem, the National Academy of Medicine (NAM) called for the development of learning healthcare systems (LHSs) which bring together clinicians, scientists, and patients work together to capture high-quality data. In essence, to turn our clinics into research labs where every patient encounter is deeply characterized. To answer this call from the NAM, we developed CHOIR (http://choir.stanford.edu) as an open-source (free) LHS. Since its introduction in 2012, we have collected many 10’s of thousands of baseline and longitudinal data on real-world patients with pain (and other health conditions). We have also implemented CHOIR in multiple academic institutions in the US, Canada and Israel. Our ultimate goal is to realize the vision of precision pain medicine where can provide the best treatment for the particular patient under a specific circumstance. Furthermore, all development efforts serve to benefit the other clinical specialties also using CHOIR. Multiple projects including machine learning, data visualization and platform development are provided in the apply link.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://forms.gle/12BK8b1FKg5pUTdV9" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>
			
			<text><font size="5">Mignot Lab: Proteomics in the CSF and Blood in Relation to Sleep Disorders</font></text><text class="base-tease__subheading">
                            <br>
                            Center for Narcolepsy, Stanford Medicine</text>
                            <p class="base-tease__subheading"><br>The Mignot  lab studies sleep and sleep disorders and we are analyzing large amounts of proteomic data (1,100 to 5,500 proteins) in the blood or CSF of patients with various pathology, together with genome wide association and exome sequencing data.   Goal is to link genetic to protein to disease.  This project requires knowledge of genetic analysis and associated statistics. Applicants who lack specialized skills in genetic analysis and statistics may still apply if they indicate willingness to commit for multiple quarters. This will allow time for mentorship and a significant contribution to the lab. Python programing needed.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://forms.gle/3TrTGiAd7ozxkatK8" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>
			
			<text><font size="5">Mignot lab: Immunogenetics and Immunology of Narcolepsy</font></text><text class="base-tease__subheading">
                            <br>
                            Center for Narcolepsy, Stanford Medicine</text>
                            <p class="base-tease__subheading"><br>Narcolepsy is an autoimmune disease, and we conducted genome wide association in over 5000 cases and exome sequencing data in many familial cases.  The disease appears to be a T cell mediated attack on hypocretin neuron elicited by a cross-reactivity with a flu antigen.  Project involves data analysis of work with T cell, Fluorescence cell sorting (FACS), sequence analysis. This project requires either (1) hands on wet lab immunology or (2) knowledge of genetic and sequence analysis and associated statistics with python programing needed. Applicants who lack specialized skills in wet lab immunology or genetic analysis may still apply if they indicate willingness to commit for multiple quarters. This will allow time for mentorship and a significant contribution to the lab.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://forms.gle/3TrTGiAd7ozxkatK8" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>
			
			<text><font size="5">Mignot Lab: Genetics and Pathophysiology of NMDA Encephalitis and Brain Paraneoplastic Syndromes</font></text><text class="base-tease__subheading">
                            <br>
                            Center for Narcolepsy, Stanford Medicine</text>
                            <p class="base-tease__subheading"><br>NMDA encephalitis is an autoimmune disease characterized by psychosis and seizure. It can occur in the context of ovarian teratomas, although more often it is not associated with any tumor.   Other paraneoplastic syndrome we study are ataxias or other brain encephalitis.  We conducted genetic analysis on large number of cases and have findings indicating specific immunological mechanism that need to be followed up through additional genetic analysis such as HLA imputation or immunological studies. Applicants who lack specialized skills in genetic analysis and statistics may still apply if they indicate willingness to commit for multiple quarters. This will allow time for mentorship and a significant contribution to the lab. Python programing needed.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://forms.gle/3TrTGiAd7ozxkatK8" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>
			
			<text><font size="5">Mignot Lab: Pathophysiology, Genetics ad Clinical Studies in Klein-Levin Syndrome (KLS)</font></text><text class="base-tease__subheading">
                            <br>
                            Center for Narcolepsy, Stanford Medicine</text>
                            <p class="base-tease__subheading"><br>This is a rare syndrome affecting children or adolescent, generally males, and characterized by periodic episodes of profound hypersomnia (sleep>20 hr/day) with cognitive abnormalities (derealization, sexual disinhibition, megaphagia).  Nothing was known about this syndrome, but through a long and painful process we have created a database of over 800 subjects and are a center of excellence for this disorder.   We conducted genetic analysis and found genes involved to be associated with bipolar disorder and circadian abnormalities.  Biomarker studies also suggest an inflammatory process.  Applicants who lack specialized skills in genetic analysis and statistics may still apply if they indicate willingness to commit for multiple quarters. This will allow time for mentorship and a significant contribution to the lab. Much remains to be done, from clinical intervention studies, to behavioral observation studies to pathophysiological studies.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://forms.gle/3TrTGiAd7ozxkatK8" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>
			
			<text><font size="5">Mignot Lab: Machine Learning for Automated Scoring of Polysomnography Studies</font></text><text class="base-tease__subheading">
                            <br>
                            Center for Narcolepsy, Stanford Medicine</text>
                            <p class="base-tease__subheading"><br>We have gathered thousands of polysomnography studies are conducting machine learning studies to score recordings automatically (supervised machine learning) , linking this data with genetics (genome wide association) or phenotype (various disease and sleep disorder). We have a team of 3-5 electrical engineers working on this and numerous projects in this space.   Requirement is ability to finish a project that is not just a demonstration project but creates something that can be built upon and includes a publication.   Python programing needed.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://forms.gle/3TrTGiAd7ozxkatK8" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>
			
				
			<text><font size="5">Gene Expression Data Organization and Mining</font></text><text class="base-tease__subheading">
                            <br>
                            Human Immune Monitoring Center, Institute for Immunity, Transplantation, and Infection</text>
                            <p class="base-tease__subheading"><br>The Human Immune Monitoring Center (HIMC) has generated gene expression data from blood cells for hundreds of healthy individuals across a range of ages, in studies profiling healthy immunity and response to influenza vaccination. Our online database, Stanford Data Miner (SDM), was recently updated to allow upload and mining of such data. This project would involve: (1) formatting existing data using an R script so that it is compatible with SDM and contains the relevant subject IDs; (2) upload of the data and mapping to subject IDs in SDM; (3) quality control and visualization of the uploaded data using novel tools in SDM; and (4) potentially defining new metrics of healthy blood gene expression from the uploaded data. Experience with running R scripts is needed, and experience with databases in general would be helpful. Academic credit can be arranged, and a commitment for 2 quarters is preferred.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://forms.gle/wFEbSFCaLSgsQzBW6" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>
				
			<text><font size="5">Machine Learning to Predict Cardiovascular Disease</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford University School of Medicine</text>
                            <p class="base-tease__subheading"><br>Corneal Arcus (CA - a white ring around the edge of the iris) has been correlated with high cholesterol particularly in younger people but not yet with coronary calcium scores. In our project, we will be recruiting subjects who have had their coronary calcium CT scans already done to capture photos of their irises using their phones.  We will then correlate the presence and severity of CA with cholesterol, coronary calcium and diabetes.  We will then train a deep learning model to predict cholesterol and coronary calcium from iris photos captured on smartphones.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://forms.gle/1Q4TW6soNQWUXQ2N8" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

				
			<text><font size="5">IMPACT Study</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford Center for Asian Health Research and Education (CARE)</text>
                            <p class="base-tease__subheading"><br>The Initiate and Maintain Physical Activity in Clinics (IMPACT) study is a 2.5-year study designed to examine the efficacy of structured exercise programs offered in the clinic setting to help patients manage their type 2 diabetes and meet exercise recommendations. Patients are assigned into one of three arms: weekly exercise 1x, 3x, or 0x (usual care). The goal is to determine which exercise regimen will most help patients adhere to exercise and lower their HbA1c values. This lab offers opportunities to conduct patient clinical visits, author papers and grants, and more. </p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://forms.gle/tFTctAR6tKcvdQup8" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

				
			<text><font size="5">STRONG-D Study</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford Center for Asian Health Research and Education (CARE)</text>
                            <p class="base-tease__subheading"><br>Normal weight individuals with type 2 diabetes (NWD) have higher risk of mortality than overweight or obese individuals. NWD may have less lean muscle mass in proportion to adipose tissue, which may contribute to this higher risk of mortality. Strength training increases muscle mass rather than reducing body weight and can combat the age-related shift in body composition that results in higher fat mass and decreased muscle mass (i.e., sarcopenia). Thus, strength training may therefore be helpful for NWD rather than weight loss, as is recommended for overweight/obese individuals with type 2 diabetes (T2DM). The goal of the Strength Training Regimen fOr Normal weiGht Diabetics (STRONG-D) Study is to determine the best exercise regimen for NWD, who experience greater mortality than overweight/obese diabetics. Patients are randomized into one of three arms: Strength, Aerobic, or Combo. This lab offers opportunities to conduct patient clinical visits, author papers and grants, and more.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://forms.gle/JJimvxKhR8CwZMEi8" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>
			
			<text><font size="5">Functional Ca2+ Image Analysis based on In Vivo Cochlear Hair Cell Imaging</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford, Department of Otolaryngology Head & Neck Surgery</text>
                            <p class="base-tease__subheading"><br>In vivo imaging with cellular resolution enables us to identify and characterize receptive field properties of a sensory system. However, cochlear in vivo imaging remain challenging, owing to not only its deep location but also bony structure filled with fluids. The goal of our project is to develop in vivo cochlea imaging to characterize how sound is transferred across sensory hair cells. So far, we’ve developed a method to resolve individual cochlear hair cells with hearing function in live mice, and are collecting functional data. We are looking for a keen research assistant at any level that is eager to learn, well organized, and that can help out with research studies on an ongoing basis. Coding skill would be preferred. This position can be for undergraduate credit or a paid position. </p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://forms.gle/Ess6bvj2XfKE2M7a7" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>


		        <text><font size="5">Machine Vision for the Neural Basis of Social Interactions</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford Department of Neurobiology</text>
                            <p class="base-tease__subheading"><br>Understanding the neural basis of social interactions remains one of the great mysteries of modern neurobiology with profound implications for basic science and neurological and psychiatric applications. This project seeks to parse videos of interacting animals into a set of behavioral building blocks tied to both individual animal actions and group dynamics. Our goal is to combine multiple deep learning approaches including convolutional neural networks and recurrent neural networks or attention-based sequence models. As a second stage the dynamics of behavior revealed by the developed algorithms will be related to neural recordings and video processing will be adapted for real-time analysis.
This project requires a two quarter minimum commitment. </p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSfyLCgNpmtP1URtFOa5jCVtHBWQ1n1BG5QxoomP4JxciWC0qA/viewform" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

                    <text><font size="5">Single Cell Transcriptomics of Tumor and Immune Biology</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford Pathology Department and VA Medical Center</text>
                            <p class="base-tease__subheading"><br>Single cell RNAseq is an emerging technology that allows us to examine expression of all genes by thousands of cells, yielding unparalleled insights into cell biology, communication and responses.  The Butcher lab is using scRNAseq to understand vascular and immune system responses and interactions in cancer and inflammation.  Student(s) will help improve existing tools for analysis and visualization of the massive data sets generated, and will help develop and apply novel algorithms for probing cell-cell interactions, gene and communication networks.   Projects will involve coding, data analysis and biological knowledge application and discovery.  Opportunities for publication and credit.  </p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSfNI49G84J1g9VAro-qkeqBkgdfW1zvKTAoYk_ov2nplPPaRQ/viewform" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

                    <text><font size="5">Early Detection of Cerebral Ischemia</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford Center for Biodesign</text>
                            <p class="base-tease__subheading"><br>Our project focuses on enabling the early detection of ischemic stroke in high-risk patients to increase access to life-saving treatments and reduce the disability and mortality associated with these often devastating events. We are looking for 2 talented students to assist us in the process of implementing state of the art algorithms for the processing of EEG signals and to contribute in the development of new methods and models for detecting cerebral ischemia. Must have: Passion to address unmet clinical needs, Fluency in Matlab and/or Python.  Nice to have: Prior experience with the Matlab Fieldtrip & EEGLAB toolkits and/or the Python MNE-Python. For more information, or to express interest, reach out to orvar@stanford.edu or ayo.roberts@stanford.edu.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSfoHYkj_FPtk5jM77SBARkGy9fyGOCdz0c0SXpXrfg-81GnJA/viewform" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

				
		    <text><font size="5">Opioid Safety: Personalizing Prescribing to Reduce Misuse</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford University, Center for Population Health Sciences; Biomedical Data Science</text>
                            <p class="base-tease__subheading"><br>How many opioid pills should a doc prescribe for a patient in pain after back surgery or knee replacement?  Does it vary by gender or age?  Opioids have run rampant in the US with some US states reporting more opioid prescriptions each year than residents.  In 2016, over 64,000 Americans died from overdoses, 21 percent more than the almost 53,000 in 2015.  If not remediated, public health experts estimate that nationwide over 500,000 people could die from the epidemic over the next 10 years.  
<br>
How can we ensure the safe and effective prescribing of one of the most effective medicines doctors have?  We seek to inform the current discussion on opioid prescribing guidelines by using a range of analytic techniques to explore the relationship between pre-surgical patient characteristics, opioid use profiles and adverse opioid outcomes.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLScOL3eCOz27LDZs02QeVzuVP5XqtUOczDRMWCXHAOWvF553dQ/viewform" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

				
			    <text><font size="5">Multi-Morbidity Clusters Associated with Mortality</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford University, Center for Population Health Sciences; Biomedical Data Science</text>
                            <p class="base-tease__subheading"><br>Danish population healthcare data is considered the "test tube" for observational health analyses.  Using a nationally representative sample of Danish residents, this work seeks to use unsupervised learning methods and data visualization techniques to examine multi-morbidity complexes that are strongly associated with one and five year mortality.  Such insights can be used in a wide range of applications from drug design to policy making.  The data has already been cleaned, processed and is ready for analysis in feature matrices.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSdE1mDKA4fsSGQHIKyU7aKa9A42KIFX4eElGZBvM7b2vNpQ_Q/viewform" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

				
                            <text><font size="5">Computer Vision for Post-Surgical Wound Surveillance</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford Medical School, Vascular Surgery</text>
                            <p class="base-tease__subheading"><br>Post-operative wound infections occur between 2% and 15%, depending on the surgery. This is considered a “never-event” and CMS does not reimburse the treatment of such complications- similar to re-admissions which occur within 30-days of a procedure. There is great benefit to identifying signs of infection early for early intervention before the typical 1-week follow up. We have an IRB approved study to collect photos from wounds from post-op day 0 through post-op day 7 which we label and have the ability to run computer vision algorithms on. In addition to early wound infection detection we are very very interested in the art of taking a photo and converting that to a “structured report” that we would use for documentation in the electronic medical record as this is what is critical for billing. In addition we have the opportunity to gain access to daily photos of patients in the ICU as well as in their rooms to automate the process of generating structured reports for documentation. An example would be, “Patient is asleep, intubated with naso-gastric tube in place.”<br>Two quarter minimum engagement (Summer internship an option)</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSdfWl9OC-Em7Eo5ljhYdc12ueoLhsaYk5RVtEZ_jbEXxkLc9Q/viewform" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>


                            <text><font size="5">Automated Carotid Ultrasound Screening Using Deep Learning</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford Medical School, Radiology</text>
                            <p class="base-tease__subheading"><br>Carotid ultrasound is a high volume screening tool to evaluate patients for stroke risk scoring. We have labeled datasets (performed by the interpreting radiologist) for normal/abnormal cases and detailed structured labels from the report dictation.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSeRQFSZLJVfrOMqBsTkyG11hTUkiCsibthDWE7Eqyr1L3hNrQ/viewform" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

                            <text><font size="5">Clinical Validation of a Passive Mobile Activity Tracking Algorithms to Assess Functional Capacity in Cardiovascular Disease Patients</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford Medical School, Vascular Surgery</text>
                            <p class="base-tease__subheading"><br>We are several years into developing a mobile research app to study and clinically validate activity tracking using a patient’s personal mobile device +/- a smart watch. In addition to getting passive daily activity data (max steps without stopping, total daily steps, flights climbed) we also have high frequency (100Hz) raw accelerometer data and heart rate data for 6-minute walk tests in well characterized patients pre- and post- interventions. We are looking for 1) response curves in activity patterns to interventions, 2) characteristics in passive metrics relative to clinical state, and 3) correlation of passive activity to 6-minute-walk-test results among other things. We have data not only form our own studies here but also from a collaborative study with a large pharmaceutical company.<br>Two quarter minimum engagement (Summer internship an option)</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSdnheQ06ql_izFU8nv_dZbFF1mkyZ5kMJh56OksoMvIh-Mq4g/viewform" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

                            <text><font size="5">Therapeutic Radiation Technology</font></text><text class="base-tease__subheading">
                            <br>
                            Zap Surgical Systems, Inc.</text>
                            <p class="base-tease__subheading"><br>Zap Surgical Systems, Inc. is a San Carlos, CA-based developer and manufacturer of a new generation of stereotactic radiosurgery systems. Zap Surgical Systems set out to create a first-of-its-kind therapeutic radiation technology that would be “self-shielded” and not require a radiation therapy vault under most circumstances. Another critical company goal was to invent a device that met or exceeded the best quality standards of existing radiosurgical instruments, i.e. there would be no compromise in terms of performance. It remains the goal of Zap Surgical Systems to provide the above revolutionary technology at the lowest possible price; the general benchmark the company has set for itself is that the cost of ownership worldwide should be approximately one-third the cost of competing radiosurgical modalities. Zap is engaged in active research on new uses for stereotactic radiosurgery. One of these is “radiomodulation,” precise, noninvasive modulation of dysfunctional brain circuits.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSf2MhZ6m7AsAQzrWiDIgJPT2kpSQuxp5eu4ae-GT5wigbciKg/viewform" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

                            <text><font size="5">Selective Plane Illumination Microscopy (SPIM)</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford Medical School, Cardiovascular Medicine</text>
                            <p class="base-tease__subheading"><br>Student(s) will assist in completing microscope hardware construction, including optomechanical integration, laser beam alignment, and computer/software integration, following the openspim design (http://openspim.org/Welcome_to_the_OpenSPIM_Wiki). After hardware construction, student(s) will create a pipeline for storing/retrieving images, videos, and associated data on Amazon Web Services (AWS) and for image processing/analysis of images, videos, and data. The data set sizes will range from a few GB upwards to TBs. Images and videos will be primarily from human pluripotent stem cell-derived tissues and organoids, along with rodent hearts and blood vessels. Experience with both hardware and software is preferred. A time commitment of at least two quarters (research credit can be arranged) is also preferred.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSexQ9vPc4NhHMBokl5kyi5NyR0tL3hDWDy34kQNj8V8Dvs2Mg/viewform" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

                            <text><font size="5">Optical Mapping Microscopy for Arrhythmia Research</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford Medical School, Cardiovascular Medicine</text>
                            <p class="base-tease__subheading"><br>Student(s) will assist in completing microscope hardware construction, including integration of a high speed camera, high power LEDs, a video projector for spatial optical illumination, optics, and a motorized XY stage. After hardware construction, student(s) will create a pipeline for storing/retrieving images, videos, and associated data on Amazon Web Services (AWS) and for image processing/analysis of images, videos, and data. Students will create data acquisition software/scripts (in LabView and/or Matlab) that integrate signals from the above devices and instruments (open loop and closed loop systems). Images and videos will be primarily from human pluripotent stem cell-derived monolayer cardiovascular tissue as a model for investigating cardiac arrhythmias. Experience with both hardware and software is preferred. A time commitment of at least two quarters (research credit can be arranged) is also preferred.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSfn6SIPLKh1X0xcDnMJl7NDNLJCmHVkvJbzJUP61ZeO_WEfIg/viewform" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

                            <text><font size="5">Segmentation of Nuclei in Histological Sections</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford University, Department of Bioengineering</text>
                            <p class="base-tease__subheading"><br>Automatic segmentation of nuclei is a key step in digital histology. While the problem has been extensively studied, there is no well-established open-source library/network. A successful project might become an essential element of many scientific workflows.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLScGHibdoBLx2jxjnYvOGZKUvMC17ARnvyX_d6FovFQC8kW-kw/viewform" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

                            <text><font size="5">Disease Treatment Proposal</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford University, Department of Bioengineering</text>
                            <p class="base-tease__subheading"><br>Design of a system that asks for symptoms. Then, based on symptoms and preprocessed embeddings of diseases (e.g. at https://en.wikipedia.org/wiki/List_of_infectious_diseases) it either diagnoses a disease or if the confidence is low, it asks for more details ("is the rash dark?"). A proof-of-concept exploratory project with many ambitious opportunities depending on interests!</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSdbyU0RRYqAhWNzYjPk9PemwR_7HbxI7Dqa3bOjMD0ocjSVBw/viewform" target="_blank" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

                            <text><font size="5">Automatic Evaluation of Burns</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford Medical School, Plastic & Reconstructive Surgery</text>
                            <p class="base-tease__subheading"><br>Care of patients with burns is highly specialized and yet initial evaluation and triage of burn patients typically occurs in centers without available burn expertise. We have assembled the largest curated dataset of burn images that have been segmented and classified by experts. During last year’s CS231n we mentored a student who developed a pixel-lever classifier to evaluate burn size and depth. Our future work includes at least two opportunities: (1) develop a classifier for normal unburned skin, which will result in a clinically useful tool that could have exciting implications for initial treatment of these critically ill patients and (2) evaluate the effect of skin tone on performance of our classifier, whether underrepresentation of certain skin tones introduces bias and impairs performance of the classifier for certain patients, and explore opportunities to enhance performance for underrepresented groups. While we cannot provide computer science expertise, we worked very closely with our student last year to provide medical subject matter expertise, which resulted in a successful project that currently is in preparation for submission to a medical journal.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSfZIt5VVA4JE1k_-Iwt-5iv3r_iimTcwoJo19YZ6c80ToGIuw/viewform" target="_blank" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

                            <text><font size="5">Receptive Field Mapping</font></text><text class="base-tease__subheading">
                            <br>
                            SLAC National Accelerator Laboratory</font></text>
                            <p class="base-tease__subheading"><br>Given a neural network trained on an image recognition task, map out the regions of input space to which each neuron responds. Our approach uses nonlinear programming to solve this as a mathematical optimization problem. Or replace “image recognition task” with the task of your choosing.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSemT7OfOqH6YTwdaai1PG_vfXkkkYu-zFj4x1yX-7cf6JnfcQ/viewform" target="_blank" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

                            <text><font size="5">Neural Network Quality Metrics</font></text><text class="base-tease__subheading">
                            <br>
                            SLAC National Accelerator Laboratory</font></text>
                            <p class="base-tease__subheading"><br>Given a trained neural network classifier, what is the rate of false positive classification? Can a statistical regularizer reduce the rate of false positives? False negatives?</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSfApifs4SdpKWPYP0NUT-_zEwlCU8YtsDSPRDIESgD_xonJYg/viewform" target="_blank" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

                            <text><font size="5">Spectral Analysis of Neural Networks</text><text class="base-tease__subheading">
                            <br>
                            SLAC National Accelerator Laboratory</font></text>
                            <p class="base-tease__subheading"><br>Given a trained neural network, what can be learned from examining the eigenvectors and eigenvalues of the weight matrices, and the Lyapunov exponents of the network? Can these measures be related (possibly by using machine learning) to the network quality metric and the receptive field maps?</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSfimcH9AEVb7V3AIR3qDz_84z9swuCakr0WUtQvEJAlOWbzDQ/viewform" target="_blank" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

                            <text><font size="5">Neural Network Explanations</text><text class="base-tease__subheading">
                            <br>
                            SLAC National Accelerator Laboratory</font></text>
                            <p class="base-tease__subheading"><br>Given a trained neural network, generate an English language description of network decisions. The approach is to use machine learning based on the information from the preceding three projects.</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSe_8jHCFgXBJvKwr2LPhsEVGAanO92v3KYONzcSXBgMRTYuFg/viewform" target="_blank" target="_blank" style="text-decoration: none;">Apply →</a></button><br><br>

                            <text><font size="5">Inferring Patient Outcomes from Healthcare Variables</font></text><text class="base-tease__subheading">
                            <br>
                            Stanford University, Department of Anesthesiology</text>
                            <p class="base-tease__subheading"><br>Use existing healthcare insurance claims data to investigate the association between healthcare variables and patient outcomes (datasets contain 10s -100s of millions of patients).</p>
                            <button style="padding: 0px 0px; font-size: 15.5px; font-weight:700; opacity:1;filter:alpha(opacity=100);background-color: #ffffff00;border: none;" onmouseout="this.style.opacity=1;this.filters.alpha.opacity=100" onmouseover="this.style.opacity=0.5;this.filters.alpha.opacity=60"><a href="https://docs.google.com/forms/d/e/1FAIpQLSdynI_mCDoXKJZ3qTkEAtKlPzRU4-1zzFtgELM7bHE_QN35gQ/viewform" target="_blank" target="_blank" style="text-decoration: none;">Apply →</a></button><br>
                        </div>
                    </div>

                </div>
                <!-- /.project-body -->

    </div>
    <!-- /.base-main-inner -->

    </div>
    <!-- /.l--main -->







    </section>
    <!-- /.l--content -->

    <!-- Slideshow -->
    <section class="base-slideshow">

        <div class="base-slideshow-inner">

            <div class="slideshow-slides"></div>

            <div class="slideshow-pagination">

                <div class="pagination-previous">
                    <button class="pagination-previous-link">Prev</button>
                </div>

                <div class="pagination-next">
                    <button class="pagination-next-link">Next</button>
                </div>

            </div>

            <div class="slideshow-close">
                <button class="slideshow-close__link">Close Slideshow</button>
            </div>

        </div>

    </section>

    <footer class="l--footer l--footer-home" id="footer">
        <div class="footer-inner">

            <button onclick="window.scrollTo({top:0,left: 0,behavior:'smooth'});" class="footer__back-to-top--link feature-theme--salmon">Back to top
      <!--?xml version="1.0" encoding="UTF-8" standalone="no"?-->
      <svg viewBox="0 0 13 52" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" class="footer__back-to-top--arrow">
        <g>
          <path d="M30.1464466,25.3535534 L-19.1464466,25.3535534 L-19.6464466,25.3535534 L-19.6464466,26.3535534 L-19.1464466,26.3535534 L30.1464466,26.3535534 L25.5,31 L25.1464466,31.3535534 L25.8535534,32.0606602 L26.2071068,31.7071068 L31.7071068,26.2071068 L32.0606602,25.8535534 L31.4571068,25.25 L31.2071068,25 L26.2071068,20 L25.8535534,19.6464466 L25.1464466,20.3535534 L25.5,20.7071068 L30.1464466,25.3535534 Z" id="arrow" transform="translate(6.207107, 25.853553) rotate(-90.000000) translate(-6.207107, -25.853553) "></path>
        </g>
        
      </svg>
    </button>


            <div id="contact" class="l--footer-home-content">

                <div class="l--left l--footer-header" style="width:125%">

                    <div class="footer-about__summary" style="max-width: 80%;">
                        <p><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3nSG30P2cMx_mNqR9oWO1zUCFAs82c2-kdJgVMwC3H9hgFA/viewform" target="_blank">Subscribe</a> to hear about new projects.<p>
                        <p>Interested in providing a project? Submit <a href="https://docs.google.com/forms/d/e/1FAIpQLSdtUUie_d-IUkvGlEuMv48SJdcNRCg3s5NFF14eIdX-75e94A/viewform?" target="_blank">here</a>.</p>
                        <p>Have questions or want to work with us?<br>Reach us at <a href="mailto:info@appliedlearninginitiative.org" target="_blank">info@appliedlearninginitiative.org</a>.</p>
                    </div>


                </div>
                <!-- /.footer-header -->


            </div>
            <!-- .l--footer-home-content -->

            <div class="l--subfooter">

                <nav class="subfooter-general">
                    <p class="subfooter-general__item--copyright">© Applied Learning Initiative. All rights reserved.</p>
                </nav>
            </div>
            <!-- /.l-subfooter -->

        </div>
    </footer>

    <div class="loading-container">
        <!--?xml version="1.0" standalone="no"?-->
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" class="loading__svg">
    <rect x="0" y="0" width="100%" height="100%" class="loading__rectangle" style="stroke-dasharray: 500, 3116; stroke-dashoffset: 3616px;"></rect>
  </svg>
    </div>

    <script type="text/javascript">
        var algolia = {"debug":false,"application_id":"UHBM7GWLCW","search_api_key":"88d85af2dc9c8ecd74bc8e1914fddad5","powered_by_enabled":false,"query":"","autocomplete":{"sources":[{"index_id":"searchable_posts","index_name":"ali_website_production_searchable_posts","label":"Searchable posts","position":10,"max_suggestions":8,"tmpl_suggestion":"autocomplete-post-suggestion","enabled":true}]},"indices":{"searchable_posts":{"name":"ali_website_production_searchable_posts","id":"searchable_posts","enabled":true,"replicas":[]}}}
    </script>
    <script type="text/html" id="tmpl-autocomplete-header">
        <div class="autocomplete-header">
            <div class="autocomplete-header-title">{{{ data.label }}}</div>
            <div class="clear"></div>
        </div>
    </script>

    <script type="text/html" id="tmpl-ali-footer">
        <div class="autocomplete-footer">
            <div class="autocomplete-footer-title"><a href="javascript:void(0);" onClick="document.getElementById('main_search').submit();">{{{ data.label }}}</a></div>
            <div class="clear"></div>
        </div>
    </script>

    <script type="text/html" id="tmpl-autocomplete-post-suggestion">
        <a class="suggestion-link" href="{{ data.permalink }}" title="{{ data.post_title }}">
            <# if ( data.images.thumbnail ) { #>
                <img class="suggestion-post-thumbnail" src="{{ data.images.thumbnail.url }}" alt="{{ data.post_title }}">
                <# } #>
                    <div class="suggestion-post-attributes">
                        <span class="suggestion-post-title">{{{ data._highlightResult.post_title.value }}}</span>

                        <# var attributes=[ 'lead_text', 'contributors', 'instructors', 'locations', 'project_types', 'publication_types', 'event_types', 'course_type', 'course_number', 'categories', 'tags', 'titles', 'websites', 'issue', 'event_series',
                            'public_lecture_series', 'content', 'topics', 'affiliations', 'departments']; var attribute_name; var relevant_content='' ; if ( 'person'==d ata.post_type && '' !=d ata.titles ) { // Special rule for Faculty/Staff: display their title(s) as the snippet,
                            regardless of context match relevant_content=d ata._snippetResult[ 'titles'].value; } else { // Show the best match that makes sense for ( var index in attributes ) { attribute_name=a ttributes[ index ]; if ( data._highlightResult[
                            attribute_name ] && data._snippetResult[ attribute_name ] ) { if ( data._highlightResult[ attribute_name ].matchedWords.length> 0 ) { relevant_content = data._snippetResult[ attribute_name ].value; break; } } } if ( '' == relevant_content ) { if ( data._snippetResult['lead_text'] && data._snippetResult['lead_text'].value ) { relevant_content = data._snippetResult['lead_text'].value;
                            } else { relevant_content = data._snippetResult['content'].value; } } } #>
                            <span class="suggestion-post-content">{{{ relevant_content }}}</span>
                    </div>
        </a>
    </script>

    <script type="text/html" id="tmpl-autocomplete-term-suggestion">
        <a class="suggestion-link" href="{{ data.permalink }}" title="{{ data.name }}">
    <span class="suggestion-post-title">{{{ data._highlightResult.name.value }}}</span>
  </a>
    </script>

    <script type="text/html" id="tmpl-autocomplete-user-suggestion">
        <a class="suggestion-link user-suggestion-link" href="{{ data.posts_url }}" title="{{ data.display_name }}">
            <# if ( data.avatar_url ) { #>
                <img class="suggestion-user-thumbnail" src="{{ data.avatar_url }}" alt="{{ data.display_name }}">
                <# } #>

                    <span class="suggestion-post-title">{{{ data._highlightResult.display_name.value }}}</span>
        </a>
    </script>

    <script type="text/html" id="tmpl-autocomplete-footer">
        <div class="autocomplete-footer">
            <div class="autocomplete-footer-branding">
                Powered by <a href="#" class="algolia-powered-by-link" title="Algolia">
        <img class="algolia-logo" src="https://www.algolia.com/assets/algolia128x40.png" alt="Algolia" />
      </a>
            </div>
        </div>
    </script>

    <script type="text/html" id="tmpl-autocomplete-empty">
        <div class="autocomplete-empty">
            No results matched your query <span class="empty-query">{{ data.query }}"</span>
        </div>
    </script>

    <script type="text/javascript">
        jQuery(function () {
            /* init Algolia client */
            var client = algoliasearch(algolia.application_id, algolia.search_api_key);
        
            /* setup default sources */
            var sources = [];
            jQuery.each(algolia.autocomplete.sources, function(i, config) {
              sources.push({
                source: autocomplete.sources.hits(client.initIndex(config['index_name']), {
                  hitsPerPage: config['max_suggestions'],
                  attributesToSnippet: [
                    'topics:10',
                    'affiliations:10',
                    'departments:10',
                    'contributors:10',
                    'locations:10',
                    'course_type:10',
                    'event_types:10',
                    'project_types:10',
                    'publication_types:10',
                    'categories:10',
                    'tags:10',
                    'titles:10',
                    'websites:10',
                    'issue:10',
                    'lead_text:10',
                    'event_series:10',
                    'public_lecture_series:10',
                    'course_number:10',
                    'content:10'
                  ]
                }),
                templates: {
        /*
                  header: function() {
                    return wp.template('autocomplete-header')({
                      label: config['label']
                    });
                  },
        */
                  header: '',
                  footer: function() {
                    return wp.template('ali-footer')({
                      label: 'All Results'
                    });
                  },
                  suggestion: wp.template(config['tmpl_suggestion'])
                }
              });
        
            });
        
            /* Setup dropdown menus */
            jQuery("input[name='s']:not('.no-autocomplete')").each(function(i) {
              var $searchInput = jQuery(this);
        
              var config = {
                debug: algolia.debug,
                // debug: true,
                hint: false,
                openOnFocus: true,
                templates: {}
              };
              /* Todo: Add empty template when we fixed https://github.com/algolia/autocomplete.js/issues/109 */
        
              if(algolia.powered_by_enabled) {
                config.templates.footer = wp.template('autocomplete-footer');
              }
        
              /* Instantiate autocomplete.js */
              autocomplete($searchInput[0], config, sources)
              .on('autocomplete:selected', function(e, suggestion, datasetName) {
                /* Redirect the user when we detect a suggestion selection. */
                window.location.href = suggestion.permalink;
              });
        
              var $autocomplete = $searchInput.parent();
        
              /* Remove autocomplete.js default inline input search styles. */
              $autocomplete.removeAttr('style');
        
              /* Configure tether */
              var $menu = $autocomplete.find('.aa-dropdown-menu');
              var config = {
                element: $menu,
                target: this,
                attachment: 'top left',
                targetAttachment: 'bottom left',
                constraints: [
                  {
                    to: 'window',
                    attachment: 'none element'
                  }
                ]
              };
        
              /* This will make sure the dropdown is no longer part of the same container as */
              /* the search input container. */
              /* It ensures styles are not overridden and limits theme breaking. */
              var tether = new Tether(config);
              tether.on('update', function(item) {
                /* todo: fix the inverse of this: https://github.com/HubSpot/tether/issues/182 */
                if (item.attachment.left == 'right' && item.attachment.top == 'top' && item.targetAttachment.left == 'left' && item.targetAttachment.top == 'bottom') {
                  config.attachment = 'top right';
                  config.targetAttachment = 'bottom right';
        
                  tether.setOptions(config, false);
                }
              });
              $searchInput.on('autocomplete:updated', function() {
                tether.position();
                // CCA 2016-12-30: Tethering to the exact container position leaves a messy 1-pixel offset.
                $menu.css('left', '-1px');
              });
              $searchInput.on('autocomplete:opened', function() {
                updateDropdownWidth();
              });
        
        
              /* Trick to ensure the autocomplete is always above all. */
              $menu.css('z-index', '99999');
        
              /* Makes dropdown match the input size. */
              var dropdownMinWidth = 200;
              function updateDropdownWidth() {
                var inputWidth = $searchInput.outerWidth();
                if (inputWidth >= dropdownMinWidth) {
                  // CCA 2016-12-30: Add 1 pixel to the menu width, and shift it left by 1 pixel (see above) for a clean finish.
                  $menu.css('width', $searchInput.outerWidth() + 1);
                } else {
                  $menu.css('width', dropdownMinWidth);
                }
                tether.position();
              }
              jQuery(window).on('resize', updateDropdownWidth);
            });
        
            /* This ensures that when the dropdown overflows the window, Thether can reposition it. */
            jQuery('body').css('overflow-x', 'hidden');
        
            jQuery(document).on("click", ".algolia-powered-by-link", function(e) {
              e.preventDefault();
              window.location = "https://www.algolia.com/?utm_source=WordPress&utm_medium=extension&utm_content=" + window.location.hostname + "&utm_campaign=poweredby";
            });
          });
    </script>
    <script type="text/javascript" src="./ALI/underscore.min.js"></script>
    <script type="text/javascript">
        /* <![CDATA[ */
        var _wpUtilSettings = {"ajax":{"url":"\/wp-admin\/admin-ajax.php"}};
        /* ]]> */
    </script>
    <script type="text/javascript" src="./ALI/wp-util.min.js"></script>
    <script type="text/javascript" src="./ALI/wp-embed.min.js"></script>


    </section>
    <!-- /.theme--* -->

    </div>
    <!-- /#pjax-container -->

    <script src="./ALI/script.js"></script>


    <div data-tether-id="1" style="top: 0px; left: 0px; position: absolute;"></div><span class="aa-dropdown-menu tether-element tether-enabled tether-abutted tether-abutted-top tether-element-attached-top tether-element-attached-left tether-target-attached-bottom tether-target-attached-left tether-abutted-left" style="position: fixed; top: 0px; z-index: 99999; display: none; left: 0px; transform: translateX(0px) translateY(0px) translateZ(0px); width: 200px;"><div class="aa-dataset-0"></div></span></body>

</html>
